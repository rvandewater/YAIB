{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b471a18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'icu_benchmarks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01micu_benchmarks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit_process_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_data\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'icu_benchmarks'"
     ]
    }
   ],
   "source": [
    "from icu_benchmarks.data.split_process_data import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9786d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(1-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f28515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dyn=pd.read_parquet('../../demo_data/los/eicu_demo/dyn.parquet', engine='pyarrow')\n",
    "outc=pd.read_parquet('../../demo_data/los/eicu_demo/outc.parquet', engine='pyarrow')\n",
    "sta=pd.read_parquet('../../demo_data/los/eicu_demo/sta.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35342dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c217e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87724d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddac54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265af37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import gin\n",
    "from torch import Tensor, cat, from_numpy, float32,empty,stack\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from typing import Dict, Tuple\n",
    "from constants import DataSegment as Segment\n",
    "from constants import DataSplit as Split\n",
    "from collections import OrderedDict\n",
    "\n",
    "FEAT_NAMES = ['s_cat' , 's_cont' , 'k_cat' , 'k_cont' , 'o_cat' , 'o_cont' , 'target', 'id']\n",
    "def ampute_data(data, mechanism, p_miss, p_obs=0.3):\n",
    "    \"\"\"\n",
    "    Generate missing values for specifics missing-data mechanism and proportion of missing values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        Data for which missing values will be simulated.\n",
    "    mechanism : str,\n",
    "            Indicates the missing-data mechanism to be used. (\"MCAR\", \"MAR\" or \"MNAR\")\n",
    "    p_miss : float\n",
    "        Proportion of missing values to generate for variables which will have missing values.\n",
    "    p_obs : float\n",
    "            If mecha = \"MAR\" or \"MNAR\", proportion of variables with *no* missing values\n",
    "            that will be used for the logistic masking model.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    imputed_data: DataFrame\n",
    "        The data with the generated missing values.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Applying {mechanism} amputation.\")\n",
    "    X = torch.tensor(data.values.astype(np.float32))\n",
    "\n",
    "    if mechanism == \"MAR\":\n",
    "        mask = MAR_logistic_mask(X, p_miss, p_obs)\n",
    "    elif mechanism == \"MNAR\":\n",
    "        mask = MNAR_logistic_mask(X, p_miss, p_obs)\n",
    "    elif mechanism == \"MCAR\":\n",
    "        mask = MCAR_mask(X, p_miss)\n",
    "    elif mechanism == \"BO\":\n",
    "        mask = BO_mask(X, p_miss)\n",
    "    else:\n",
    "        logging.error(\"Not a valid amputation mechanism. Missing-data mechanisms to be used are MCAR, MAR or MNAR.\")\n",
    "\n",
    "    amputed_data = data.mask(mask)\n",
    "    return amputed_data, mask\n",
    "class CommonDataset(Dataset):\n",
    "    \"\"\"Common dataset: subclass of Torch Dataset that represents the data to learn on.\n",
    "\n",
    "    Args: data: Dict of the different splits of the data. split: Either 'train','val' or 'test'. vars: Contains the names of\n",
    "    columns in the data. grouping_segment: str, optional: The segment of the data contains the grouping column with only\n",
    "    unique values. Defaults to Segment.outcome. Is used to calculate the number of stays in the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: dict,\n",
    "        split: str = Split.train,\n",
    "        vars: Dict[str, str] = gin.REQUIRED,\n",
    "        grouping_segment: str = Segment.outcome,\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.vars = vars\n",
    "        self.grouping_df = data[split][grouping_segment].set_index(self.vars[\"GROUP\"])\n",
    "        self.features_df = (\n",
    "            #drops time coulmn and sets index to stay_id\n",
    "            data[split][Segment.features].set_index(self.vars[\"GROUP\"]).drop(labels=self.vars[\"SEQUENCE\"], axis=1)\n",
    "        )\n",
    "\n",
    "        # calculate basic info for the data\n",
    "        self.num_stays = self.grouping_df.index.unique().shape[0]\n",
    "        self.maxlen = self.features_df.groupby([self.vars[\"GROUP\"]]).size().max()\n",
    "\n",
    "    def ram_cache(self, cache: bool = True):\n",
    "        self._cached_dataset = None\n",
    "        if cache:\n",
    "            logging.info(\"Caching dataset in ram.\")\n",
    "            self._cached_dataset = [self[i] for i in range(len(self))]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns number of stays in the data.\n",
    "\n",
    "        Returns:\n",
    "            number of stays in the data\n",
    "        \"\"\"\n",
    "        return self.num_stays\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.features_df.columns\n",
    "\n",
    "    def to_tensor(self):\n",
    "        values = []\n",
    "        for entry in self:\n",
    "            for i, value in enumerate(entry):\n",
    "                if len(values) <= i:\n",
    "                    values.append([])\n",
    "                values[i].append(value.unsqueeze(0))\n",
    "        return [cat(value, dim=0) for value in values]\n",
    "\n",
    "\n",
    "@gin.configurable(\"PredictionDataset\")\n",
    "class PredictionDataset(CommonDataset):\n",
    "    \"\"\"Subclass of common dataset for prediction tasks.\n",
    "\n",
    "    Args:\n",
    "        ram_cache (bool, optional): Whether the complete dataset should be stored in ram. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, ram_cache: bool = True, **kwargs):\n",
    "        super().__init__(*args, grouping_segment=Segment.outcome, **kwargs)\n",
    "        self.outcome_df = self.grouping_df\n",
    "        self.ram_cache(ram_cache)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Function to sample from the data split of choice. Used for deep learning implementations.\n",
    "\n",
    "        Args:\n",
    "            idx: A specific row index to sample.\n",
    "\n",
    "        Returns:\n",
    "            A sample from the data, consisting of data, labels and padding mask.\n",
    "        \"\"\"\n",
    "        if self._cached_dataset is not None:\n",
    "            return self._cached_dataset[idx]\n",
    "\n",
    "        pad_value = 0.0\n",
    "        stay_id = self.outcome_df.index.unique()[idx]  # [self.vars[\"GROUP\"]]\n",
    "\n",
    "        # slice to make sure to always return a DF\n",
    "        window = self.features_df.loc[stay_id:stay_id].to_numpy()\n",
    "        labels = self.outcome_df.loc[stay_id:stay_id][self.vars[\"LABEL\"]].to_numpy(dtype=float)\n",
    "\n",
    "        if len(labels) == 1:\n",
    "            # only one label per stay, align with window\n",
    "            labels = np.concatenate([np.empty(window.shape[0] - 1) * np.nan, labels], axis=0)\n",
    "\n",
    "        length_diff = self.maxlen - window.shape[0]\n",
    "        \n",
    "        \n",
    "        pad_mask = np.ones(window.shape[0])\n",
    "\n",
    "        # Padding the array to fulfill size requirement\n",
    "        if length_diff > 0:\n",
    "            # window shorter than the longest window in dataset, pad to same length\n",
    "            window = np.concatenate([window, np.ones((length_diff, window.shape[1])) * pad_value], axis=0)\n",
    "            labels = np.concatenate([labels, np.ones(length_diff) * pad_value], axis=0)\n",
    "            pad_mask = np.concatenate([pad_mask, np.zeros(length_diff)], axis=0)\n",
    "\n",
    "        not_labeled = np.argwhere(np.isnan(labels))\n",
    "        if len(not_labeled) > 0:\n",
    "            labels[not_labeled] = -1\n",
    "            pad_mask[not_labeled] = 0\n",
    "\n",
    "        pad_mask = pad_mask.astype(bool)\n",
    "        labels = labels.astype(np.float32)\n",
    "        data = window.astype(np.float32)\n",
    "\n",
    "        return from_numpy(data), from_numpy(labels), from_numpy(pad_mask)\n",
    "\n",
    "    def get_balance(self) -> list:\n",
    "        \"\"\"Return the weight balance for the split of interest.\n",
    "\n",
    "        Returns:\n",
    "            Weights for each label.\n",
    "        \"\"\"\n",
    "        counts = self.outcome_df[self.vars[\"LABEL\"]].value_counts()\n",
    "        return list((1 / counts) * np.sum(counts) / counts.shape[0])\n",
    "\n",
    "    def get_data_and_labels(self) -> Tuple[np.array, np.array]:\n",
    "        \"\"\"Function to return all the data and labels aligned at once.\n",
    "\n",
    "        We use this function for the ML methods which don't require an iterator.\n",
    "\n",
    "        Returns:\n",
    "            A Tuple containing data points and label for the split.\n",
    "        \"\"\"\n",
    "        labels = self.outcome_df[self.vars[\"LABEL\"]].to_numpy().astype(float)\n",
    "        rep = self.features_df\n",
    "        if len(labels) == self.num_stays:\n",
    "            # order of groups could be random, we make sure not to change it\n",
    "            rep = rep.groupby(level=self.vars[\"GROUP\"], sort=False).last()\n",
    "        rep = rep.to_numpy().astype(float)\n",
    "        \n",
    "        return rep, labels\n",
    "\n",
    "    def to_tensor(self):\n",
    "        data, labels = self.get_data_and_labels()\n",
    "        return from_numpy(data).to(float32), from_numpy(labels).to(float32)\n",
    "\n",
    "@gin.configurable(\"PredictionDatasetTFT\")\n",
    "class PredictionDatasetTFT(PredictionDataset):\n",
    "    \"\"\"Subclass of prediction dataset for TFT as we need to define if variables are cont,static,known or observed.\n",
    "\n",
    "    Args:\n",
    "        ram_cache (bool, optional): Whether the complete dataset should be stored in ram. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, ram_cache: bool = True, **kwargs):\n",
    "        super().__init__(*args,ram_cache = True, **kwargs)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Function to sample from the data split of choice. Used for TFT.\n",
    "\n",
    "        Args:\n",
    "            idx: A specific row index to sample.\n",
    "\n",
    "        Returns:\n",
    "            A sample from the data, consisting of data, labels and padding mask.\n",
    "        \"\"\"\n",
    "        if self._cached_dataset is not None:\n",
    "            return self._cached_dataset[idx]\n",
    "\n",
    "        pad_value = 0.0\n",
    "        stay_id = self.outcome_df.index.unique()[idx]  # [self.vars[\"GROUP\"]]\n",
    "\n",
    "        \n",
    "        # We need to be sure that tensors are returned in the correct order to be processed correclty by tft\n",
    "        tensors = [[] for _ in range(8)]\n",
    "        for var in self.features_df.columns:\n",
    "            if  var == 'sex' :\n",
    "                tensors[0].append(self.features_df.loc[stay_id:stay_id][var].to_numpy())\n",
    "            elif var == 'age' or var== 'height' or var== 'weight':\n",
    "                tensors[1].append(self.features_df.loc[stay_id:stay_id][var].to_numpy())\n",
    "            else :\n",
    "                tensors[5].append(self.features_df.loc[stay_id:stay_id][var].to_numpy())\n",
    "    \n",
    "        \n",
    "        tensors[6].extend(self.outcome_df.loc[stay_id:stay_id][self.vars[\"LABEL\"]].to_numpy(dtype=float))\n",
    "        tensors[7].append(np.asarray([stay_id]))\n",
    "        \n",
    "        window_shape0=np.shape(tensors[0])[1]\n",
    "       # window = self.features_df.loc[stay_id:stay_id].to_numpy()\n",
    "       # labels = self.outcome_df.loc[stay_id:stay_id][self.vars[\"LABEL\"]].to_numpy(dtype=float)\n",
    "        if len(tensors[6]) == 1:\n",
    "            # only one label per stay, align with window\n",
    "            tensors[6] = np.concatenate([np.empty(window_shape0 - 1) * np.nan, tensors[6]], axis=0)\n",
    "\n",
    "        length_diff = self.maxlen - window_shape0\n",
    "        pad_mask = np.ones(window_shape0)\n",
    "\n",
    "        # Padding the array to fulfill size requirement\n",
    "        if length_diff > 0:\n",
    "            # window shorter than the longest window in dataset, pad to same length\n",
    "            tensors[0]= np.concatenate([tensors[0], np.ones((length_diff, np.shape(tensors[0])[0])) * pad_value], axis=0)\n",
    "            tensors[1]= np.concatenate([tensors[1], np.ones((length_diff, np.shape(tensors[1])[0])) * pad_value], axis=0)\n",
    "          #  tensors[2]= np.concatenate([tensors[2], np.ones((length_diff, np.shape(tensors[2])[0])) * pad_value], axis=0)\n",
    "          #  tensors[3]= np.concatenate([tensors[3], np.ones((length_diff, np.shape(tensors[3])[0])) * pad_value], axis=0)\n",
    "          #  tensors[4]= np.concatenate([tensors[4], np.ones((length_diff, np.shape(tensors[4])[0])) * pad_value], axis=0)\n",
    "            tensors[5]= np.concatenate([tensors[5], np.ones((length_diff, np.shape(tensors[5])[0])) * pad_value], axis=0)\n",
    "            tensors[7]= np.concatenate([tensors[7], np.ones((length_diff, np.shape(tensors[7])[0])) * stay_id], axis=0)\n",
    "           # window = np.concatenate([window, np.ones((length_diff, window.shape[1])) * pad_value], axis=0)\n",
    "            \n",
    "            tensors[6] = np.concatenate([tensors[6], np.ones(length_diff) * pad_value], axis=0)\n",
    "            pad_mask = np.concatenate([pad_mask, np.zeros(length_diff)], axis=0)\n",
    "\n",
    "        not_labeled = np.argwhere(np.isnan(tensors[6]))\n",
    "        if len(not_labeled) > 0:\n",
    "            tensors[6][not_labeled] = -1\n",
    "            pad_mask[not_labeled] = 0\n",
    "        tensors[6]=[tensors[6]]\n",
    "        pad_mask = pad_mask.astype(bool)\n",
    "       # data = window.astype(np.float32)\n",
    "        tensors = (from_numpy(np.array(tensor)).to(float32) for tensor in tensors)\n",
    "        tensors = [stack((x,), dim=-1) if x.numel() > 0 else empty(0) for x in tensors]\n",
    "        \n",
    "        return  OrderedDict(zip(FEAT_NAMES, tensors)),from_numpy(pad_mask)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@gin.configurable(\"ImputationDataset\")\n",
    "class ImputationDataset(CommonDataset):\n",
    "    \"\"\"Subclass of Common Dataset that contains data for imputation models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Dict[str, DataFrame],\n",
    "        split: str = Split.train,\n",
    "        vars: Dict[str, str] = gin.REQUIRED,\n",
    "        mask_proportion=0.3,\n",
    "        mask_method=\"MCAR\",\n",
    "        mask_observation_proportion=0.3,\n",
    "        ram_cache: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Dict[str, DataFrame]): data to use\n",
    "            split (str, optional): split to apply. Defaults to Split.train.\n",
    "            vars (Dict[str, str], optional): contains names of columns in the data. Defaults to gin.REQUIRED.\n",
    "            mask_proportion (float, optional): proportion to artificially mask for amputation. Defaults to 0.3.\n",
    "            mask_method (str, optional): masking mechanism. Defaults to \"MCAR\".\n",
    "            mask_observation_proportion (float, optional): poportion of the observed data to be masked. Defaults to 0.3.\n",
    "            ram_cache (bool, optional): if the dataset should be completely stored in ram and not generated on the fly during\n",
    "                training. Defaults to True.\n",
    "        \"\"\"\n",
    "        super().__init__(data, split, vars, grouping_segment=Segment.static)\n",
    "        self.amputated_values, self.amputation_mask = ampute_data(\n",
    "            self.features_df, mask_method, mask_proportion, mask_observation_proportion\n",
    "        )\n",
    "        self.amputation_mask = (self.amputation_mask + self.features_df.isna().values).bool()\n",
    "        self.amputation_mask = DataFrame(self.amputation_mask, columns=self.vars[Segment.dynamic])\n",
    "        self.amputation_mask[self.vars[\"GROUP\"]] = self.features_df.index\n",
    "        self.amputation_mask.set_index(self.vars[\"GROUP\"], inplace=True)\n",
    "\n",
    "        self.target_missingness_mask = self.features_df.isna()\n",
    "        self.features_df.fillna(0, inplace=True)\n",
    "        self.ram_cache(ram_cache)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Function to sample from the data split of choice.\n",
    "\n",
    "        Used for deep learning implementations.\n",
    "\n",
    "        Args:\n",
    "            idx: A specific row index to sample.\n",
    "\n",
    "        Returns:\n",
    "            A sample from the data, consisting of data, labels and padding mask.\n",
    "        \"\"\"\n",
    "        if self._cached_dataset is not None:\n",
    "            return self._cached_dataset[idx]\n",
    "        stay_id = self.grouping_df.iloc[idx].name\n",
    "\n",
    "        # slice to make sure to always return a DF\n",
    "        window = self.features_df.loc[stay_id:stay_id, self.vars[Segment.dynamic]]\n",
    "        window_missingness_mask = self.target_missingness_mask.loc[stay_id:stay_id, self.vars[Segment.dynamic]]\n",
    "        amputated_window = self.amputated_values.loc[stay_id:stay_id, self.vars[Segment.dynamic]]\n",
    "        amputation_mask = self.amputation_mask.loc[stay_id:stay_id, self.vars[Segment.dynamic]]\n",
    "\n",
    "        return (\n",
    "            from_numpy(amputated_window.values).to(float32),\n",
    "            from_numpy(amputation_mask.values).to(float32),\n",
    "            from_numpy(window.values).to(float32),\n",
    "            from_numpy(window_missingness_mask.values).to(float32),\n",
    "        )\n",
    "\n",
    "\n",
    "@gin.configurable(\"ImputationPredictionDataset\")\n",
    "class ImputationPredictionDataset(Dataset):\n",
    "    \"\"\"Subclass of torch dataset that represents data with missingness for imputation.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): dict of the different splits of the data\n",
    "        grouping_column (str, optional): column that is used for grouping. Defaults to \"stay_id\".\n",
    "        select_columns (List[str], optional): the columns to serve as input for the imputation model. Defaults to None.\n",
    "        ram_cache (bool, optional): wether the dataset should be stored in ram. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: DataFrame,\n",
    "        grouping_column: str = \"stay_id\",\n",
    "        select_columns: List[str] = None,\n",
    "        ram_cache: bool = True,\n",
    "    ):\n",
    "        self.dyn_df = data\n",
    "\n",
    "        if select_columns is not None:\n",
    "            self.dyn_df = self.dyn_df[list(select_columns) + grouping_column]\n",
    "\n",
    "        if grouping_column is not None:\n",
    "            self.dyn_df = self.dyn_df.set_index(grouping_column)\n",
    "        else:\n",
    "            self.dyn_df = data\n",
    "\n",
    "        # calculate basic info for the data\n",
    "        self.group_indices = self.dyn_df.index.unique()\n",
    "        self.maxlen = self.dyn_df.groupby(grouping_column).size().max()\n",
    "\n",
    "        self._cached_dataset = None\n",
    "        if ram_cache:\n",
    "            logging.info(\"Caching dataset in ram.\")\n",
    "            self._cached_dataset = [self[i] for i in range(len(self))]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns number of stays in the data.\n",
    "\n",
    "        Returns:\n",
    "            number of stays in the data\n",
    "        \"\"\"\n",
    "        return self.group_indices.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Function to sample from the data split of choice.\n",
    "\n",
    "        Used for deep learning implementations.\n",
    "\n",
    "        Args:\n",
    "            idx: A specific row index to sample.\n",
    "\n",
    "        Returns:\n",
    "            A sample from the data, consisting of data, labels and padding mask.\n",
    "        \"\"\"\n",
    "        if self._cached_dataset is not None:\n",
    "            return self._cached_dataset[idx]\n",
    "        stay_id = self.group_indices[idx]\n",
    "\n",
    "        # slice to make sure to always return a DF\n",
    "        window = self.dyn_df.loc[stay_id:stay_id, :]\n",
    "\n",
    "        return from_numpy(window.values).to(float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "504fd6c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'\n  In call to configurable 'PredictionDataset' (<class '__main__.PredictionDataset'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPredictionDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdyn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m scope_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in scope \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scope_str) \u001b[38;5;28;01mif\u001b[39;00m scope_str \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1604\u001b[0m err_str \u001b[38;5;241m=\u001b[39m err_str\u001b[38;5;241m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> 1605\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_exception_message_and_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[38;5;241m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(exception)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mwith_traceback(exception\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mPredictionDataset.__init__\u001b[0;34m(self, ram_cache, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, ram_cache: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouping_segment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutcome_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_df\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mram_cache(ram_cache)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mCommonDataset.__init__\u001b[0;34m(self, data, split, vars, grouping_segment)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m split\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mvars\u001b[39m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m[grouping_segment]\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvars[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROUP\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m#drops time coulmn and sets index to stay_id\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     data[split][Segment\u001b[38;5;241m.\u001b[39mfeatures]\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvars[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROUP\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mdrop(labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvars[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# calculate basic info for the data\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'\n  In call to configurable 'PredictionDataset' (<class '__main__.PredictionDataset'>)"
     ]
    }
   ],
   "source": [
    "train_dataset = PredictionDataset(dyn, split=Split.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f48f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaib",
   "language": "python",
   "name": "yaib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
